{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "from urllib.parse import urlparse\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class URLCrawler():\n",
    "    def __init__(self, categories) -> None:\n",
    "        if type(categories) !=  list:\n",
    "            self.categories = [categories]\n",
    "            print(self.categories)\n",
    "        else:\n",
    "            self.categories = categories\n",
    "        self.path = './Data/'\n",
    "        self.makeDirectory()\n",
    "\n",
    " \n",
    "    def checkURL(self, requested_url):\n",
    "        if not urlparse(requested_url).scheme:\n",
    "            requested_url = \"https://\" + requested_url\n",
    "        return requested_url\n",
    "\n",
    "\n",
    "    def requestAndParse(self, requested_url):\n",
    "        requested_url = self.checkURL(requested_url)\n",
    "        try:\n",
    "            # define headers to be provided for request authentication\n",
    "            headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) ' \n",
    "                            'AppleWebKit/537.11 (KHTML, like Gecko) '\n",
    "                            'Chrome/23.0.1271.64 Safari/537.11',\n",
    "                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "                'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "                'Accept-Encoding': 'none',\n",
    "                'Accept-Language': 'en-US,en;q=0.8',\n",
    "                'Connection': 'keep-alive'}\n",
    "            request_obj = Request(url=requested_url, headers=headers)\n",
    "            page_html = urlopen(request_obj)\n",
    "            page_soup = BeautifulSoup(page_html, \"html.parser\")\n",
    "            return page_soup\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    def crawlImageURL(self, soupObject):\n",
    "        collection = []\n",
    "        soup = soupObject\n",
    "        # Find all tag 'img' to find images. \n",
    "        for img in soup.find_all('img'):\n",
    "            # Take the url of each image. \n",
    "            collection.append(img.attrs['src'])\n",
    "        collection = set(collection) #Use set to remove duplicate values.\n",
    "        return list(collection)\n",
    "\n",
    "    def makeDirectory(self):\n",
    "        for name in self.categories:\n",
    "            path = os.path.join(self.path, name)\n",
    "            if not os.path.exists(path): \n",
    "                os.mkdir(path)\n",
    "                print('{} IS CREATED!!!'.format(name))\n",
    "            else:\n",
    "                print('{} IS EXIST!!!'.format(path))\n",
    "\n",
    "    def downloadImgWithURL(self, urlCollection, category):\n",
    "\n",
    "        for url in urlCollection:\n",
    "            indexes = re.split('/|\\?', url)\n",
    "            for idx in indexes:\n",
    "                # If the image has its name on the url, use that name, else, use the format below. \n",
    "                if re.search('\\d\\.(jpg|png|jpeg)', idx):\n",
    "                    filename = idx\n",
    "                else:\n",
    "                    filename = category + '_' + str(hash(np.random.randint(0, 100000))) + '_' + str(hash(np.random.randint(0, 100000))) + '.jpg'\n",
    "                    # filename = idx[-2] + '.jpg'\n",
    "                    \n",
    "            response = requests.get(url, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                # response.raw.decode_content = True\n",
    "                open(self.path + category + '/' + filename, 'wb').write(response.content)\n",
    "\n",
    "    def download(self, website):\n",
    "        urlList = []\n",
    "        if type(website) != list:\n",
    "            urlList.append(website)\n",
    "        else:\n",
    "            urlList = website\n",
    "\n",
    "        for web in website:\n",
    "            print('CRAWLING AT ' + web)\n",
    "            for category in tqdm(self.categories):\n",
    "                print('CRAWLING IMAGE OF ' + category.upper())\n",
    "                soupObject = self.requestAndParse(web + category + '/')\n",
    "                imgCollection = self.crawlImageURL(soupObject)\n",
    "                self.downloadImgWithURL(imgCollection, category)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat IS CREATED!!!\n",
      "lion IS CREATED!!!\n",
      "leopard IS CREATED!!!\n",
      "tiger IS CREATED!!!\n",
      "jaguar IS CREATED!!!\n",
      "sphynx IS CREATED!!!\n",
      "dog IS CREATED!!!\n",
      "wolf IS CREATED!!!\n",
      "husky IS CREATED!!!\n",
      "corgi IS CREATED!!!\n",
      "pug IS CREATED!!!\n",
      "CRAWILING AT https://www.pexels.com/search/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRAWLING IMAGE OF CAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1/11 [00:10<01:45, 10.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRAWLING IMAGE OF LION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 2/11 [00:17<01:14,  8.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRAWLING IMAGE OF LEOPARD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 3/11 [00:24<01:03,  7.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRAWLING IMAGE OF TIGER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 4/11 [00:35<01:03,  9.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRAWLING IMAGE OF JAGUAR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 5/11 [00:42<00:50,  8.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRAWLING IMAGE OF SPHYNX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 6/11 [00:49<00:39,  7.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRAWLING IMAGE OF DOG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 7/11 [00:59<00:34,  8.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRAWLING IMAGE OF WOLF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 8/11 [01:09<00:26,  8.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRAWLING IMAGE OF HUSKY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 9/11 [01:15<00:16,  8.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRAWLING IMAGE OF CORGI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 10/11 [01:22<00:07,  7.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRAWLING IMAGE OF PUG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [01:30<00:00,  8.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRAWILING AT https://unsplash.com/s/photos/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRAWLING IMAGE OF CAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1/11 [00:16<02:43, 16.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRAWLING IMAGE OF LION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 2/11 [00:44<03:28, 23.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRAWLING IMAGE OF LEOPARD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 3/11 [01:15<03:34, 26.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRAWLING IMAGE OF TIGER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 4/11 [01:38<02:58, 25.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRAWLING IMAGE OF JAGUAR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 5/11 [02:06<02:37, 26.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRAWLING IMAGE OF SPHYNX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 6/11 [02:42<02:27, 29.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRAWLING IMAGE OF DOG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 7/11 [03:07<01:52, 28.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRAWLING IMAGE OF WOLF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 8/11 [03:37<01:25, 28.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRAWLING IMAGE OF HUSKY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 9/11 [04:02<00:55, 27.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRAWLING IMAGE OF CORGI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 10/11 [04:29<00:27, 27.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRAWLING IMAGE OF PUG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [05:00<00:00, 27.35s/it]\n"
     ]
    }
   ],
   "source": [
    "url_resources = ['https://www.pexels.com/search/', 'https://unsplash.com/s/photos/']\n",
    "\n",
    "animal_list = ['cat', 'lion', 'leopard', 'tiger', 'jaguar', 'sphynx', \n",
    "                'dog', 'wolf', 'husky', 'corgi', 'pug']\n",
    "\n",
    "crawling = URLCrawler(animal_list)\n",
    "crawling.download(url_resources)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
