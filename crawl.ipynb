{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class URLCrawler():\n",
    "    def __init__(self, categories) -> None:\n",
    "        if type(categories) !=  list:\n",
    "            self.categories = [categories]\n",
    "            print(self.categories)\n",
    "        else:\n",
    "            self.categories = categories\n",
    "        self.path = './Data/'\n",
    "        self.makeDirectory()\n",
    "\n",
    " \n",
    "    def checkURL(self, requested_url):\n",
    "        if not urlparse(requested_url).scheme:\n",
    "            requested_url = \"https://\" + requested_url\n",
    "        return requested_url\n",
    "\n",
    "\n",
    "    def requestAndParse(self, requested_url):\n",
    "        requested_url = self.checkURL(requested_url)\n",
    "        try:\n",
    "            # define headers to be provided for request authentication\n",
    "            headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) ' \n",
    "                            'AppleWebKit/537.11 (KHTML, like Gecko) '\n",
    "                            'Chrome/23.0.1271.64 Safari/537.11',\n",
    "                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "                'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "                'Accept-Encoding': 'none',\n",
    "                'Accept-Language': 'en-US,en;q=0.8',\n",
    "                'Connection': 'keep-alive'}\n",
    "            request_obj = Request(url=requested_url, headers=headers)\n",
    "            page_html = urlopen(request_obj)\n",
    "            page_soup = BeautifulSoup(page_html, \"html.parser\")\n",
    "            return page_soup\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    def crawlImageURL(self, soupObject):\n",
    "        collection = []\n",
    "        soup = soupObject\n",
    "        for img in soup.find_all('img'):\n",
    "            collection.append(img.attrs['src'])\n",
    "        collection = set(collection) #Use set to remove duplicate values.\n",
    "        return list(collection)\n",
    "\n",
    "    def makeDirectory(self):\n",
    "        for name in self.categories:\n",
    "            path = os.path.join(self.path, name)\n",
    "            if not os.path.exists(path): \n",
    "                os.mkdir(path)\n",
    "                print('{} IS CREATED!!!'.format(name))\n",
    "            else:\n",
    "                print('{} IS EXIST!!!'.format(path))\n",
    "\n",
    "    def downloadImgWithURL(self, urlCollection, category):\n",
    "\n",
    "        for url in urlCollection:\n",
    "            indexes = re.split('/|\\?', url)\n",
    "            for idx in indexes:\n",
    "                if re.search('\\d\\.(jpg|png|jpeg)', idx):\n",
    "                    filename = idx\n",
    "                else:\n",
    "                    filename = category + '_' + str(hash(np.random.randint(0, 100000))) + '_' + str(hash(np.random.randint(0, 100000))) + '.jpg'\n",
    "                    # filename = idx[-2] + '.jpg'\n",
    "                    \n",
    "            response = requests.get(url, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                response.raw.decode_content = True\n",
    "                open(self.path + category + '/' + filename, 'wb').write(response.content)\n",
    "\n",
    "    def download(self, website):\n",
    "        urlList = []\n",
    "        if type(website) != list:\n",
    "            urlList.append(website)\n",
    "        else:\n",
    "            urlList = website\n",
    "\n",
    "        for web in website:\n",
    "            print('CRAWILING AT ' + web)\n",
    "            for category in self.categories:\n",
    "                print('CRAWLING IMAGE OF ' + category.upper())\n",
    "                soupObject = self.requestAndParse(web + category + '/')\n",
    "                imgCollection = self.crawlImageURL(soupObject)\n",
    "                self.downloadImgWithURL(imgCollection, category)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [06:36<00:00, 36.06s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# soupObject = requestAndParse('https://www.freeimages.com/search/cat')\n",
    "# imgCollection = crawlImageURL(soupObject)\n",
    "# imgCollection\n",
    "\n",
    "# for animal in tqdm(animal_list):\n",
    "#     soupObject = requestAndParse(url_resources[0] + animal + '/')\n",
    "#     imgCollection = crawlImageURL(soupObject)\n",
    "#     downloadImgWithURL(imgCollection, animal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat IS CREATED!!!\n",
      "lion IS CREATED!!!\n",
      "leopard IS CREATED!!!\n",
      "tiger IS CREATED!!!\n",
      "jaguar IS CREATED!!!\n",
      "sphynx IS CREATED!!!\n",
      "dog IS CREATED!!!\n",
      "wolf IS CREATED!!!\n",
      "husky IS CREATED!!!\n",
      "corgi IS CREATED!!!\n",
      "pug IS CREATED!!!\n",
      "DOWNLOADING IMAGE OF CAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRAWLING AT https://www.pexels.com/search/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:10<00:20, 10.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRAWLING AT https://www.freeimages.com/search/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:10<00:21, 10.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 410: Gone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [50], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m animal_list \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mcat\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlion\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mleopard\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtiger\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mjaguar\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msphynx\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m      4\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mdog\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwolf\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhusky\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcorgi\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpug\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m crawling \u001b[39m=\u001b[39m URLCrawler(animal_list)\n\u001b[0;32m----> 7\u001b[0m crawling\u001b[39m.\u001b[39mdownload(url_resources)\n",
      "Cell \u001b[0;32mIn [49], line 83\u001b[0m, in \u001b[0;36mURLCrawler.download\u001b[0;34m(self, website)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mCRAWLING AT \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m web)\n\u001b[1;32m     82\u001b[0m soupObject \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequestAndParse(web \u001b[39m+\u001b[39m category \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 83\u001b[0m imgCollection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcrawlImageURL(soupObject)\n\u001b[1;32m     84\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownloadImgWithURL(imgCollection, category)\n",
      "Cell \u001b[0;32mIn [49], line 41\u001b[0m, in \u001b[0;36mURLCrawler.crawlImageURL\u001b[0;34m(self, soupObject)\u001b[0m\n\u001b[1;32m     39\u001b[0m collection \u001b[39m=\u001b[39m []\n\u001b[1;32m     40\u001b[0m soup \u001b[39m=\u001b[39m soupObject\n\u001b[0;32m---> 41\u001b[0m \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m soup\u001b[39m.\u001b[39;49mfind_all(\u001b[39m'\u001b[39m\u001b[39mimg\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     42\u001b[0m     collection\u001b[39m.\u001b[39mappend(img\u001b[39m.\u001b[39mattrs[\u001b[39m'\u001b[39m\u001b[39msrc\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     43\u001b[0m collection \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(collection) \u001b[39m#Use set to remove duplicate values.\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "url_resources = ['https://www.pexels.com/search/', 'https://www.freeimages.com/search/', 'https://unsplash.com/s/photos/']\n",
    "\n",
    "animal_list = ['cat', 'lion', 'leopard', 'tiger', 'jaguar', 'sphynx', \n",
    "                'dog', 'wolf', 'husky', 'corgi', 'pug']\n",
    "\n",
    "crawling = URLCrawler(animal_list)\n",
    "crawling.download(url_resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CAT_1096'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# soupObject = crawling.requestAndParse('cat')\n",
    "# imgCollection = crawling.crawlImageURL(soupObject)\n",
    "# crawling.downloadImgWithURL(imgCollection, 'cat')\n",
    "\n",
    "# imgCollection\n",
    "'CAT_' + str(hash(np.random.randint(0, 10000)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
