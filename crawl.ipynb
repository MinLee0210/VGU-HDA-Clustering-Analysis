{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkURL(requested_url):\n",
    "    if not urlparse(requested_url).scheme:\n",
    "        requested_url = \"https://\" + requested_url\n",
    "    return requested_url\n",
    "\n",
    "\n",
    "def requestAndParse(requested_url):\n",
    "    requested_url = checkURL(requested_url)\n",
    "    try:\n",
    "        # define headers to be provided for request authentication\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) ' \n",
    "                        'AppleWebKit/537.11 (KHTML, like Gecko) '\n",
    "                        'Chrome/23.0.1271.64 Safari/537.11',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "            'Accept-Encoding': 'none',\n",
    "            'Accept-Language': 'en-US,en;q=0.8',\n",
    "            'Connection': 'keep-alive'}\n",
    "        request_obj = Request(url=requested_url, headers=headers)\n",
    "        page_html = urlopen(request_obj)\n",
    "        page_soup = BeautifulSoup(page_html, \"html.parser\")\n",
    "        return page_soup\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "def crawlImageURL(soupObject):\n",
    "    collection = []\n",
    "    soup = soupObject\n",
    "    for img in soup.find_all('img'):\n",
    "        collection.append(img.attrs['src'])\n",
    "    collection = set(collection) #Use set to remove duplicate values.\n",
    "    return list(collection)\n",
    "\n",
    "def makeDirectory(parent_dir, directoryName):\n",
    "    path = os.path.join(parent_dir, directoryName)\n",
    "    if not path: \n",
    "        os.mkdir(path)\n",
    "        print('{} is created!!!'.format(directoryName))\n",
    "    else:\n",
    "        print('{} is exist!!!'.format(path))\n",
    "\n",
    "def downloadImgWithURL(urlCollection, category):\n",
    "\n",
    "    for url in urlCollection:\n",
    "\n",
    "        indexes = re.split('/|\\?', url)\n",
    "        for idx in indexes:\n",
    "            if re.search('\\d\\.(jpg|png|jpeg)', idx):\n",
    "                filename = idx\n",
    "                \n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            response.raw.decode_content = True\n",
    "            open('./Data/' + filename, 'wb').write(response.content)\n",
    "            # if os.path.isfile('./Data/' + category):\n",
    "            #     open('./Data/' + filename, 'wb').write(response.content)\n",
    "            # else:\n",
    "            #     makeDirectory('./Data/', category)\n",
    "            #     open('./Data/' + category + '/' + filename, 'wb').write(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [06:36<00:00, 36.06s/it]\n"
     ]
    }
   ],
   "source": [
    "url_resources = ['https://www.pexels.com/search/', 'https://www.freeimages.com/search/']\n",
    "\n",
    "animal_list = ['cat', 'lion', 'leopard', 'tiger', 'jaguar', 'sphynx', \n",
    "                'dog', 'wolf', 'husky', 'corgi', 'pug']\n",
    "\n",
    "# soupObject = requestAndParse('https://www.freeimages.com/search/cat')\n",
    "# imgCollection = crawlImageURL(soupObject)\n",
    "# imgCollection\n",
    "\n",
    "for animal in tqdm(animal_list):\n",
    "    soupObject = requestAndParse(url_resources[0] + animal + '/')\n",
    "    imgCollection = crawlImageURL(soupObject)\n",
    "    downloadImgWithURL(imgCollection, animal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 308: Permanent Redirect\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [123], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m soupObject \u001b[39m=\u001b[39m requestAndParse(\u001b[39m'\u001b[39m\u001b[39mhttps://www.pexels.com/search/cat\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m imgCollection \u001b[39m=\u001b[39m crawlImageURL(soupObject)\n\u001b[1;32m      3\u001b[0m imgCollection\n",
      "Cell \u001b[0;32mIn [117], line 30\u001b[0m, in \u001b[0;36mcrawlImageURL\u001b[0;34m(soupObject)\u001b[0m\n\u001b[1;32m     28\u001b[0m collection \u001b[39m=\u001b[39m []\n\u001b[1;32m     29\u001b[0m soup \u001b[39m=\u001b[39m soupObject\n\u001b[0;32m---> 30\u001b[0m \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m tqdm(soup\u001b[39m.\u001b[39;49mfind_all(\u001b[39m'\u001b[39m\u001b[39mimg\u001b[39m\u001b[39m'\u001b[39m)):\n\u001b[1;32m     31\u001b[0m     collection\u001b[39m.\u001b[39mappend(img\u001b[39m.\u001b[39mattrs[\u001b[39m'\u001b[39m\u001b[39msrc\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     32\u001b[0m collection \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(collection) \u001b[39m#Use set to remove duplicate values.\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "soupObject = requestAndParse('https://www.pexels.com/search/cat')\n",
    "imgCollection = crawlImageURL(soupObject)\n",
    "imgCollection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
